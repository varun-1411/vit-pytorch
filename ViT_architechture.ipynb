{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch embedding module.\n",
    "    Splits the image into non-overlapping patches and then embeds them.\n",
    "\n",
    "    Parameters:\n",
    "    - in_channels: Number of input channels\n",
    "    - patch_size: Size of the patch\n",
    "    - emb_size: Size of the embedding (after the patch embedding)\n",
    "    - img_size: Size of the image (image has to be square)\n",
    "\n",
    "    Attributes:\n",
    "    - n_patches: Number of patches inside a single image\n",
    "    - projection: Convolutional layer that does both the splitting into patches and the embedding\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size, in_channels=3, patch_size = 16 , emb_size=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the PatchEmbed module.\n",
    "        \n",
    "        Parameters:\n",
    "        x : torch.Tensor.Shape (n_samples, in_channels, img_size, img_size)\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor.Shape (n_samples, n_patches, emb_size)\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.projection(x) # (n_samples, emb_size, n_patches ** 0.5, n_patches ** 0.5)\n",
    "        x = x.flatten(2) # (n_samples, emb_size, n_patches)\n",
    "        x = x.transpose(1, 2) # (n_samples, n_patches, emb_size)\n",
    "        return x\n",
    "    \n",
    "class Attention(nn.module):\n",
    "    \"\"\"\n",
    "    Attention module.\n",
    "    \n",
    "    Parameters:\n",
    "    - emb_size: Size of the embedding (input and out dimension of per token features)\n",
    "    - n_heads: Number of attention heads\n",
    "    -drop_prob_qkv: Dropout rate to apply to the query, key and value tensors\n",
    "    - drop_prob_o: Dropout rate after the softmax layer\n",
    "    -qkv_bias: Whether to include bias in the qkv projection layers\n",
    "    - causal: Whether to apply causal masking or not\n",
    "    \n",
    "    Attributes:\n",
    "    - scales: Precomputed square root of the head_dim\n",
    "    - qkv: Linear layer for the query, key and value\n",
    "    - proj_qkv: linear mapping that takes in concatenated output of all heads and projects it back to new space\n",
    "    - dropout_qkv, dropout_o: Dropout layer\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emb_size, n_heads = 12, drop_prob_o = 0.,drop_prob_qkv = 0., qkv_bias = True):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = emb_size // n_heads\n",
    "        assert self.head_dim * n_heads == emb_size, \"Embedding size should be divisible by the number of heads\"\n",
    "        self.scales = self.head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3, bias = qkv_bias) # q, k, v\n",
    "        self.dropout_qkv = nn.Dropout(drop_prob_qkv)\n",
    "        self.proj_qkv = nn.Linear(emb_size, emb_size)\n",
    "        self.dropout_o = nn.Dropout(drop_prob_o)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the Attention module.\n",
    "        \n",
    "        Parameters:\n",
    "        x : torch.Tensor.Shape (n_samples, n_patches + 1, emb_size)\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor.Shape (n_samples, n_patches + 1, emb_size)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        n_samples, n_tokens, emb_size = x.shape\n",
    "        if emb_size != self.emb_size:\n",
    "            raise ValueError(f\"Attention expects {self.emb_size} input features, but got {emb_size}\")\n",
    "        qkv = self.qkv(x).reshape(n_samples, n_tokens, 3, self.n_heads, self.head_dim) # (n_samples, n_patches + 1, 3 * dim), (n_samples, n_patches + 1, 3, n_heads, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k_t = k.transpose(-2, -1) # (n_samples, n_heads, head_dim, n_patches + 1)\n",
    "        dots = (q @ k_t) * self.scales # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "        attn = dots.softmax(dim=-1) # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "        attn = self.dropout_kqv(attn)\n",
    "        weighted_avg = attn @ v # (n_samples, n_heads, n_patches + 1, head_dim)\n",
    "        weighted_avg = weighted_avg.transpose(1, 2) # (n_samples, n_patches + 1, n_heads, head_dim)\n",
    "        weighted_avg = weighted_avg.flatten(2) # (n_samples, n_patches + 1, emb_size)\n",
    "        x = self.proj_qkv(weighted_avg) # (n_samples, n_patches + 1, emb_size)\n",
    "        x = self.dropout_o(x)\n",
    "        return x\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP module.\n",
    "\n",
    "    Parameters:\n",
    "    - in_features: Number of input features\n",
    "    - hidden_features: Number of hidden layer features\n",
    "    - out_features: Number of output features\n",
    "    - drop_prob: Dropout rate to apply\n",
    "\n",
    "    Attributes:\n",
    "    - fc1, fc2: Linear layers -> nn.Linear(in_features, hidden_features), nn.Linear(hidden_features, out_features)\n",
    "    - act_fn: GELU activation function -> nn.GELU()\n",
    "    - dropout: Dropout layer -> nn.Dropout(drop_prob)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features, out_features, drop_prob = 0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act_fn = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the MLP module.\n",
    "        \n",
    "        Parameters:\n",
    "        x : torch.Tensor.Shape (n_samples, n_patches + 1, emb_size or input_features)\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor.Shape (n_samples, n_patches + 1, emb_size or out_features)\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.fc1(x) # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.act_fn(x) # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.dropout(x) # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.fc2(x) # (n_samples, n_patches + 1, out_features)\n",
    "        x = self.dropout(x) # (n_samples, n_patches + 1, out_features)  # here out_features are same as hidden_features\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block.\n",
    "\n",
    "    Parameters:\n",
    "    - emb_size: Size of the input embedding\n",
    "    - n_heads: Number of attention heads\n",
    "    - mlp_ratio: Multiplier for the hidden dim of the MLP wrt the input embedding\n",
    "    - qkv_bias: Whether to include bias in the qkv projection layers\n",
    "    - drop_prob: Dropout rate to apply\n",
    "    - attn_drop_prob: Dropout rate to apply to the attention module\n",
    "\n",
    "    Attributes:\n",
    "    - norm1, norm2: LayerNorms  # Layer norm makes sure that the mean of each sample is 0 and the standard deviation is 1\n",
    "    - attn: Attention module\n",
    "    - mlp: MLP module\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_size, n_heads, mlp_ratio = 4., qkv_bias = True, drop_prob = 0., attn_drop_prob = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(emb_size, eps=1e-6)\n",
    "        self.attn = Attention(emb_size, n_heads, drop_prob_o = attn_drop_prob, drop_prob_qkv = attn_drop_prob, qkv_bias = qkv_bias)\n",
    "        self.norm2 = nn.LayerNorm(emb_size, eps=1e-6)\n",
    "        hidden_features = int(emb_size * mlp_ratio)\n",
    "        self.mlp = MLP(emb_size, hidden_features, emb_size, drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the Block module.\n",
    "        \n",
    "        Parameters:\n",
    "        x : torch.Tensor.Shape (n_samples, n_patches + 1, emb_size)\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor.Shape (n_samples, n_patches + 1, emb_size)\n",
    "\n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer.\n",
    "\n",
    "    Parameters:\n",
    "    - img_size: Size of the image (image has to be square)\n",
    "    - patch_size: Size of the patch\n",
    "    - in_channels: Number of input channels\n",
    "    - n_classes: Number of output classes\n",
    "    - emb_size: Size of the token/patch embedding\n",
    "    - depth: Number of transformer blocks\n",
    "    - n_heads: Number of attention heads\n",
    "    - mlp_ratio: Multiplier for the hidden dim of the MLP wrt the input embedding\n",
    "    - qkv_bias: Whether to include bias in the qkv projection layers\n",
    "    - drop_prob: Dropout rate to apply\n",
    "    - attn_drop_prob: Dropout rate to apply to the attention module\n",
    "\n",
    "    Attributes:\n",
    "    - patch_embed: PatchEmbed module\n",
    "    - cls_token: Learnable parameter that will represent the whole image or 1st token in sequence.\n",
    "    - pos_embed: Positional embedding of cls token + all the patches and it has (n_patches + 1)* emb_size elements.\n",
    "    - pos_drop: Dropout layer\n",
    "    - blocks: Sequence of the transformer blocks\n",
    "    - norm: Layer norm\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size = 384, patch_size = 16, in_channels = 3, n_classes = 1000, emb_size = 768, depth = 12, n_heads = 12, mlp_ratio = 4., qkv_bias = True, drop_prob = 0., attn_drop_prob = 0.):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, in_channels, patch_size, emb_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 1 + self.patch_embed.n_patches, emb_size))\n",
    "        self.pos_drop = nn.Dropout(drop_prob)\n",
    "        self.blocks = nn.ModuleList([Block(emb_size, n_heads, mlp_ratio, qkv_bias, drop_prob, attn_drop_prob) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(emb_size, eps=1e-6)\n",
    "        self.head = nn.Linear(emb_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the VisionTransformer module.\n",
    "        \n",
    "        Parameters:\n",
    "        x : torch.Tensor.Shape (n_samples, in_channels, img_size, img_size)\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor.Shape (n_samples, n_classes) logits over all the classes\n",
    "\n",
    "        \"\"\"\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(n_samples, -1, -1) # (n_samples, 1, emb_size)\n",
    "        x = torch.cat((cls_token, x), dim=1) # (n_samples, 1 + n_patches, emb_size)\n",
    "        x = x + self.pos_embed # (n_samples, 1 + n_patches, emb_size)\n",
    "        x = self.pos_drop(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]\n",
    "        x = self.head(cls_token_final)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import timm \n",
    "from custom import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to count number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def assert_tensors_equal(t1, t2):\n",
    "    assert t1.shape == t2.shape, f\"Shapes do not match: {t1.shape} and {t2.shape}\"\n",
    "    assert torch.allclose(t1, t2, atol=1e-4), \"Tensors do not match\"\n",
    "\n",
    "model_name = \"vit_base_patch16_384\"\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "custom_config = {\"img_size\": 384, \"patch_size\": 16, \"in_channels\": 3, \"n_classes\": 1000, \"emb_size\": 768, \"depth\": 12, \"n_heads\": 12, \"mlp_ratio\": 4.0, \"qkv_bias\": True, \"drop_prob\": 0.0, \"attn_drop_prob\": 0.0}\n",
    "\n",
    "custom_model = VisionTransformer(**custom_config)\n",
    "custom_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8924, -1.2784, -0.6579, -1.6928, -0.0461]],\n",
       "\n",
       "        [[-0.8924, -1.2784, -0.6579, -1.6928, -0.0461]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 1, 5).expand(2, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = torch.nn.Dropout(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in module.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elementwise affine = False means that the layer will not learn the affine parameters (bias and scale)\n",
    "# only last dimension is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "n_samples, n_features = inp.shape\n",
    "module = torch.nn.LayerNorm(n_features, elementwise_affine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2.], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(inp).mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "module.bias.data += 1\n",
    "module.weight.data += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3,2,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.flatten(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout(p=0.4, inplace=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
